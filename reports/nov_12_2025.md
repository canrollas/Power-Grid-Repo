# Multi-Client LSTM Load Forecasting – Experiment & Literature Context

This section summarizes **what I did** in my experiment and **what exists in the literature** using the *ElectricityLoadDiagrams20112014* dataset.

---

## 1. What I Did – Multi-Client LSTM Experiment

In this study, I implemented a **multi-client short-term load forecasting model** on the UCI *ElectricityLoadDiagrams20112014* dataset. Since the dataset itself is already well known, I focus on the **modeling pipeline and experimental setup** rather than basic data description.

### 1.1 Multi-Client Problem Formulation

* Each client (household / meter) is treated as one time series.
* Instead of training a separate model per client, I train **one shared model** for **370 valid clients**.
* The model takes a **sliding window of past load values** and predicts the **next time step**:

  * **Input sequence length:** 24 time steps
  * **Forecast horizon:** 1 step ahead
* A strictly **time-based split** is used to avoid information leakage:

  * Earlier years are used for training,
  * Later periods are used as validation and test.

Cold-start clients with extremely short or problematic histories are removed in preprocessing, so the model only sees series with sufficient historical information.

### 1.2 Data Preparation and Scaling

The preprocessing pipeline operates across all clients jointly:

* After removing invalid / short series, the pipeline reports:

  * **Number of valid clients:** 370
  * **Scaler fitted on:** 33,484,131 training data points
  * **Created train sequences:** 33,475,251
  * **Created test sequences:** 8,362,495
* A global scaler (e.g. standardization or min–max) is fitted on the training portion only and applied to all clients.
* Sliding windows of length 24 are created for each client, paired with the next value as the prediction target.

This produces a large multi-client dataset where each sample is:

```text
(client_id, past 24 normalized loads) -> next normalized load
```

### 1.3 Model Architecture

I use a compact but expressive **multi-client LSTM** architecture:

* **Client embedding:**
  Each of the 370 clients is mapped to a learned embedding of dimension **8**.
* **Sequence model:**
  Single-layer **LSTM with 32 units**.
* **Output head:**
  Fully connected layer mapping the LSTM output to a single continuous value (next-step load).
* **Total number of trainable parameters:** **8,369**.

The client embedding allows the model to learn client-specific behavior (e.g. typical consumption level, shape of daily profile) while still sharing temporal dynamics across all clients.

### 1.4 Training Setup and Hardware

Training is performed on a **Ubuntu** system with a dedicated GPU.

From the logs:

* **Available devices:**
  `['/physical_device:CPU:0', '/physical_device:GPU:0']`
* **Device types:** `['CPU', 'GPU']`
* **Selected device:**
  `/physical_device:GPU:0 (NVIDIA GeForce RTX 4060 Ti, 6070 MB)`
* **CUDA:** GPU acceleration is explicitly enabled:

  * `CUDA GPU acceleration enabled`
  * TensorFlow log shows device creation with compute capability 8.9.

Training configuration:

* **Epochs:** 20
* **Batch size:** 512
* **Mixed precision:** FP16 enabled for faster GPU training.
* **Checkpointing:**
  Best model saved automatically each epoch to:

```text
data/processed/models/best_lstm_model.keras
```

Example training log on the **scaled** data:

* Epoch 1:
  `loss: 1.216e-06, mae: 3.73e-04, val_loss: 7.30e-05, val_mae: 1.47e-03`
* Epoch 2–4:
  Training loss / MAE further decrease to around `~1.05e-06` and `~3.2e-04`,
  while validation MAE stays around `1.5–1.6e-03`.
* After epoch 5, the learning rate is reduced from 0.001 to 0.0005.

These values are on the **normalized scale**, so absolute magnitudes are small; the physically meaningful error is interpreted in kW after inverse scaling.

### 1.5 Test Performance

After training, the best checkpoint is evaluated on the held-out test set. The reported metrics (in physical units) are:

* **MAE:** 55.51 kW
* **RMSE:** 411.07 kW
* **MAPE:** 39.43%
* **R²:** 0.9888

The high **R²** indicates that the model explains most of the global variance in load, whereas the relatively large **MAPE** suggests that the model still struggles on low-load periods or on specific clients where percentage error becomes large. This discrepancy is typical in multi-client settings where absolute loads differ significantly between clients.

The experiment pipeline also generates a **comprehensive HTML report** summarizing the run:

* Report path:

```text
data/processed/experiments/experiment_20251123_230811/report.html
```

* Experiment folder:

```text
data/processed/experiments/experiment_20251123_230811
```

This automatically documents model configuration, data statistics, and evaluation metrics.

---
## 2. What the Literature Does with This Dataset

To position this experiment, I reviewed recent works that use the **UCI ElectricityLoadDiagrams20112014** dataset for load forecasting. Main patterns:

- Many papers work on **aggregated** or **hourly** versions of the data.
- They often incorporate **seasonal decomposition**, **calendar features** (hour of day, weekday/weekend, holidays), and sometimes **temperature** or external variables.
- Architectures include **Graph Attention Networks**, **Transformers**, and **hybrid CNN–LSTM + attention** models.
- Reported errors on aggregate tasks are often in the **MAPE ≈ 2–3%** range. These numbers are not directly comparable to my per-client, 15-minute, multi-series setting, but they provide a performance reference.

### 2.1 Graph Attention + Temporal ConvNet (Huang et al., 2023)

- **Paper:** *Multidimensional Feature-Based Graph Attention Networks and Dynamic Learning for Electricity Load Forecasting*  
  **Link:** https://www.mdpi.com/1996-1073/16/18/6443
- **Goal:** day-ahead load forecasting (24 hours ahead at 15-minute resolution) for the **aggregated load of 370 clients**.
- **Model:** proposed **TCN_DLGAT**, combining Temporal Convolutional Networks, differential learning, and Graph Attention; compared against LSTM, CNN-LSTM, LSTNet, Transformer, and TCN baselines.
- **Features / preprocessing:** STL decomposition (trend/seasonality), holiday indicators, first-order differences, 7-day sliding window to predict the next 24 hours.
- **Results:** around **2.88% MAPE**, about 24% lower than the next-best model; graph-attention and differential features lead to 5–26% error reduction over other deep models.

**Relation to my work:**  
This paper focuses on **aggregate load** with rich feature engineering and a relatively complex architecture. My current experiment uses a single LSTM on **multi-client data** with historical load and client embeddings only, so it can be seen as a simpler, global baseline without graph structure or decomposition.

### 2.2 End-to-End Imputation + Forecasting with Missing Data (Tran et al., 2023)

- **Paper:** *An End-to-End Time Series Model for Simultaneous Imputation and Forecast*  
  **Link (arXiv):** https://arxiv.org/abs/2306.00778
- **Goal:** multi-step forecasting on multivariate time series with **up to 40% missing data**, evaluated on the Electricity dataset.
- **Model:** dual-network architecture that jointly **imputes missing values and forecasts** future load; compared against Autoformer, Informer, FEDformer and other Transformer-based models.
- **Metrics:** MAE and MSE.
- **Preprocessing:** handles DST anomalies, zero-filled periods for clients without data; uses normalization and random missing masks during training.

**Relation to my work:**  
My pipeline currently assumes a **clean dataset after removing cold-start clients** and does not explicitly model missing data. The Tran et al. architecture suggests a next step: adding **joint imputation + forecasting** to better exploit partially observed clients instead of discarding them.

### 2.3 Latent Space Forecast Enhancer – Cold-Start (Heidrich et al., 2022)

- **Paper:** *Boost short-term load forecasts with synthetic data from transferred latent space information*  
  **Link:** https://energyinformatics.springeropen.com/articles/10.1186/s42162-022-00214-7
- **Goal:** day-ahead hourly load forecasting for **individual buildings** under **cold-start** conditions (very limited historical data), using Electricity client series aggregated to hourly resolution.
- **Model (LSFE):** generative model (cINN or cVAE) pre-trained on other buildings to produce synthetic load data for a new building; then trains a forecasting model (CNN, FCN, or LSTM) on the enriched dataset.
- **Metrics:** RMSE and RMSE Skill Score relative to persistence.

**Relation to my work:**  
In my current experiment, **cold-start clients are removed**. LSFE shows a principled way to **keep** them by generating synthetic histories and may be integrated later if we want a more realistic scenario where new clients continuously enter the system.

### 2.4 Deep Model Comparisons for Day-Ahead Load (Pelekis et al., 2023)

- **Paper:** *A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers*  
  **Link (journal):** https://www.sciencedirect.com/science/article/pii/S2352467723001790  
  **Link (arXiv):** https://arxiv.org/abs/2302.12168
- **Goal:** national-level day-ahead load forecasting (24 hourly steps) for Portugal’s power system, using historical load only.
- **Models compared:** MLP, LSTM, N-BEATS, TCN, Temporal Fusion Transformer (TFT), plus daily and weekly naïve seasonal benchmarks.
- **Metrics:** mainly **MAPE** (also RMSE, sMAPE, MASE).
- **Key finding:** **N-BEATS** achieves ≈**1.90% MAPE** and a simple MLP around **2.3% MAPE**, rivaling more complex models; errors are higher during early morning and holidays.

**Relation to my work:**  
Although this is a **national aggregate** task (not multi-client), it is useful as a **performance reference**. It also motivates testing **N-BEATS or simple MLP baselines** on aggregate versions of my data for comparison.

### 2.5 LSTM vs. Transformer on Electricity (Sarkar & Chu, 2025)

- **Paper:** *A Comparative Study of LSTM Efficiency vs. Transformer Power for Localized Time Series Forecasting*  
  **Link (PDF):** https://annals-csis.org/proceedings/2025/pliks/7041.pdf
- **Goal:** multivariate short-term forecasting on several benchmark datasets including the Electricity dataset, treating each client as a variable and predicting up to 12 hours ahead.
- **Models:** encoder–decoder LSTM, attention-based LSTM, and several Transformer-style architectures (Autoformer, FEDformer, Informer).
- **Result (high-level):** attention-augmented LSTMs can outperform Transformers on short-horizon tasks, indicating that **LSTM-based models remain competitive** in this regime.

**Relation to my work:**  
My model is a simpler global LSTM (with client embeddings). This aligns with their observation that LSTMs are strong for short horizons. It also suggests that adding **attention over time or over clients** could be a natural next improvement.

### 2.6 Hybrid Seasonal Decomposition + Deep Learning (Gao et al., 2024/2025)

- **Paper:** *A Novel Residential Electricity Load Prediction Algorithm Based on Hybrid Seasonal Decomposition and Deep Learning Models*  
  **Link (SSRN):** https://ssrn.com/abstract=5066249  
  (DOI: http://dx.doi.org/10.2139/ssrn.5066249)
- **Goal:** residential electricity load prediction (hourly load curve and daily total consumption).
- **Model:** seasonal decomposition combined with a **CNN-LSTM + attention** hybrid model; compared to SVR, standalone CNN, LSTM, GRU, vanilla CNN-LSTM, and Autoformer.
- **Results:** achieves MAPE ≈**2.36%** on hourly forecasts and **0.76%** on daily totals, outperforming all baselines.
- **Key idea:** explicitly modeling seasonal patterns before feeding the data into deep models significantly improves accuracy.

**Relation to my work:**  
Currently, my pipeline does **not** apply seasonal decomposition or attention. This suggests that adding **seasonal decomposition + attention** to my multi-client LSTM (or CNN-LSTM) could be a strong direction to reduce error, especially on daily patterns and seasonality.

---

## 3. Positioning of My Experiment

* My current work implements a **unified multi-client LSTM baseline** with:

  * 370 clients,
  * client embeddings,
  * 24-step input window,
  * 1-step-ahead forecasting at high resolution,
  * GPU-accelerated training with mixed precision.
* In contrast, much of the literature:

  * Works on **aggregated or hourly** data,
  * Uses **richer feature sets** (calendar / holiday / temperature, seasonal decomposition),
  * Employs more sophisticated architectures (Graph Attention Networks, dual imputation–forecast networks, N-BEATS, hybrid CNN–LSTM with attention).

Therefore, this experiment can be presented as:

> A first global LSTM baseline on 370 clients with client embeddings and time-based splitting, which achieves high R² but relatively high MAPE. Literature indicates that adding seasonal decomposition, calendar features, graph/attention structures, or synthetic data for cold start should significantly reduce errors, and these are natural next steps on top of this baseline.
